from typing import List, Dict, Optional, Any, TypedDict
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage
from config import Config, EYEWEAR_KEYWORDS
import re
import json
from datetime import datetime
from utils.embedding_manager import EmbeddingManager
from utils.qdrant_manager import QdrantManager

# ƒê·ªãnh nghƒ©a State cho LangGraph
class RAGState(TypedDict):
    """State cho RAG workflow v·ªõi LangGraph"""
    # Input
    query: str
    user_context: Dict[str, Any]
    
    # Processing
    intent_info: Dict[str, Any]
    query_embedding: Optional[List[float]]
    retrieved_documents: List[Dict]
    relevant_documents: List[Dict]
    context: str
    
    # Output
    answer: str
    sources: List[str]
    confidence_score: float
    
    # Metadata
    messages: List[BaseMessage]
    step: str
    processing_time: float
    status: str
    error: Optional[str]

class RAGAgent:
    def __init__(self):
        """
        Kh·ªüi t·∫°o RAG Agent v·ªõi LangGraph workflow v√† streaming support
        """
        print(f"ü§ñ ƒêang kh·ªüi t·∫°o RAG Agent v·ªõi LangGraph cho domain: {Config.DOMAIN}")
        print(f"ü§ñ ƒêang kh·ªüi t·∫°o Gemini model: {Config.GEMINI_MODEL}")
        
        self.llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=Config.GEMINI_TEMPERATURE,
            max_output_tokens=Config.GEMINI_MAX_OUTPUT_TOKENS,
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        # Kh·ªüi t·∫°o c√°c managers (s·∫Ω ƒë∆∞·ª£c inject t·ª´ workflow)
        self.embedding_manager = None
        self.qdrant_manager = None
        
        # T·∫°o LangGraph workflow
        self.workflow = self._create_workflow()
        self.compiled_workflow = self.workflow.compile()
        
        print("‚úÖ RAG Agent v·ªõi LangGraph ƒë√£ s·∫µn s√†ng!")
    
    def set_managers(self, embedding_manager: EmbeddingManager, qdrant_manager: QdrantManager):
        """Inject c√°c managers c·∫ßn thi·∫øt v·ªõi validation"""
        if embedding_manager is None:
            raise ValueError("Embedding manager kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        if qdrant_manager is None:
            raise ValueError("Qdrant manager kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
            
        self.embedding_manager = embedding_manager
        self.qdrant_manager = qdrant_manager
        
        print("‚úÖ ƒê√£ inject managers v√†o RAG Agent th√†nh c√¥ng")
        print(f"   - Embedding model: {getattr(embedding_manager, 'model', 'unknown')}")
        print(f"   - Qdrant collection: {getattr(qdrant_manager, 'collection_name', 'unknown')}")
    
    def _create_workflow(self) -> StateGraph:
        """T·∫°o LangGraph workflow cho RAG process"""
        workflow = StateGraph(RAGState)
        
        # Th√™m c√°c nodes
        workflow.add_node("detect_intent", self.detect_intent_node)
        workflow.add_node("retrieve_documents", self.retrieve_documents_node)
        workflow.add_node("filter_documents", self.filter_documents_node)
        workflow.add_node("aggregate_context", self.aggregate_context_node)
        workflow.add_node("generate_answer", self.generate_answer_node)
        workflow.add_node("post_process", self.post_process_node)
        workflow.add_node("handle_error", self.handle_error_node)
        
        # ƒê·ªãnh nghƒ©a entry point
        workflow.set_entry_point("detect_intent")
        
        # ƒê·ªãnh nghƒ©a edges
        workflow.add_edge("detect_intent", "retrieve_documents")
        workflow.add_edge("retrieve_documents", "filter_documents")
        workflow.add_edge("filter_documents", "aggregate_context")
        workflow.add_edge("aggregate_context", "generate_answer")
        workflow.add_edge("generate_answer", "post_process")
        workflow.add_edge("post_process", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    def detect_intent_node(self, state: RAGState) -> RAGState:
        """Node ph√¢n t√≠ch intent c·ªßa c√¢u h·ªèi"""
        try:
            state["step"] = "Ph√¢n t√≠ch intent"
            state["messages"] = add_messages(state.get("messages", []), 
                                           [HumanMessage(content=f"ƒêang ph√¢n t√≠ch intent cho: {state['query']}")])
            
            intent_info = self.detect_query_intent(state["query"])
            state["intent_info"] = intent_info
            state["status"] = "intent_detected"
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói ph√¢n t√≠ch intent: {str(e)}"
            state["status"] = "error"
            return state
    
    def retrieve_documents_node(self, state: RAGState) -> RAGState:
        """Node truy xu·∫•t documents t·ª´ vector store"""
        try:
            state["step"] = "Truy xu·∫•t documents"
            state["messages"] = add_messages(state.get("messages", []), 
                                           [HumanMessage(content="ƒêang t√¨m ki·∫øm th√¥ng tin li√™n quan...")])
            
            if not self.embedding_manager or not self.qdrant_manager:
                raise Exception("Embedding manager ho·∫∑c Qdrant manager ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p")
            
            # T·∫°o embedding cho query
            query_embedding = self.embedding_manager.embed_query(state["query"])
            state["query_embedding"] = query_embedding.tolist()  # Convert numpy array to list
            
            # T√¨m ki·∫øm documents - S·ª≠a method name v√† convert data type
            retrieved_docs = self.qdrant_manager.search_similar_documents(
                query_embedding.tolist(),  # Convert numpy array to list for Qdrant
                limit=Config.TOP_K_DOCUMENTS
            )
            state["retrieved_documents"] = retrieved_docs
            state["status"] = "documents_retrieved"
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói truy xu·∫•t documents: {str(e)}"
            state["status"] = "error"
            return state
    
    def filter_documents_node(self, state: RAGState) -> RAGState:
        """Node l·ªçc v√† ƒë√°nh gi√° documents"""
        try:
            state["step"] = "L·ªçc documents"
            state["messages"] = add_messages(state.get("messages", []), 
                                           [HumanMessage(content="ƒêang ƒë√°nh gi√° ƒë·ªô li√™n quan c·ªßa t√†i li·ªáu...")])
            
            relevant_docs = self.grade_retrieved_documents(state["query"], state["retrieved_documents"])
            state["relevant_documents"] = relevant_docs
            state["status"] = "documents_filtered"
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói l·ªçc documents: {str(e)}"
            state["status"] = "error"
            return state
    
    def aggregate_context_node(self, state: RAGState) -> RAGState:
        """Node t·ªïng h·ª£p context t·ª´ documents"""
        try:
            state["step"] = "T·ªïng h·ª£p context"
            state["messages"] = add_messages(state.get("messages", []), 
                                           [HumanMessage(content="ƒêang t·ªïng h·ª£p th√¥ng tin...")])
            
            context = self.aggregate_context(state["relevant_documents"])
            enhanced_context = self.enhance_context_with_keywords(context, state["intent_info"])
            
            state["context"] = enhanced_context
            state["sources"] = list(set([doc["source"] for doc in state["relevant_documents"]]))
            state["status"] = "context_aggregated"
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói t·ªïng h·ª£p context: {str(e)}"
            state["status"] = "error"
            return state
    
    def generate_answer_node(self, state: RAGState) -> RAGState:
        """Node t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi LLM"""
        try:
            state["step"] = "T·∫°o c√¢u tr·∫£ l·ªùi"
            state["messages"] = add_messages(state.get("messages", []), 
                                           [HumanMessage(content="ƒêang t·∫°o c√¢u tr·∫£ l·ªùi...")])
            
            # T·∫°o prompt domain-specific
            prompt = self.create_domain_prompt(state["query"], state["context"], state["intent_info"])
            
            # G·ªçi LLM
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            
            state["answer"] = answer
            state["confidence_score"] = 0.8  # C√≥ th·ªÉ t√≠nh to√°n d·ª±a tr√™n context quality
            state["status"] = "answer_generated"
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}"
            state["status"] = "error"
            return state
    
    def post_process_node(self, state: RAGState) -> RAGState:
        """Node x·ª≠ l√Ω h·∫≠u k·ª≥ response"""
        try:
            state["step"] = "Ho√†n thi·ªán"
            
            # Post-process response
            final_answer = self.post_process_response(state["answer"], state["intent_info"])
            state["answer"] = final_answer
            
            # Th√™m AI message v√†o conversation
            state["messages"] = add_messages(state.get("messages", []), 
                                           [AIMessage(content=final_answer)])
            
            state["status"] = "completed"
            state["processing_time"] = 0.0  # S·∫Ω ƒë∆∞·ª£c t√≠nh ·ªü workflow level
            
            return state
        except Exception as e:
            state["error"] = f"L·ªói x·ª≠ l√Ω h·∫≠u k·ª≥: {str(e)}"
            state["status"] = "error"
            return state
    
    def handle_error_node(self, state: RAGState) -> RAGState:
        """Node x·ª≠ l√Ω l·ªói"""
        error_message = f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {state.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}"
        
        state["answer"] = error_message
        state["messages"] = add_messages(state.get("messages", []), 
                                       [AIMessage(content=error_message)])
        state["status"] = "error_handled"
        
        return state
    
    def invoke(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """
        Invoke synchronous processing
        """
        initial_state: RAGState = {
            "query": query,
            "user_context": user_context or {},
            "intent_info": {},
            "query_embedding": None,
            "retrieved_documents": [],
            "relevant_documents": [],
            "context": "",
            "answer": "",
            "sources": [],
            "confidence_score": 0.0,
            "messages": [],
            "step": "Completed",
            "processing_time": 0.0,
            "status": "completed",
            "error": None
        }
        
        try:
            start_time = datetime.now()
            final_state = self.compiled_workflow.invoke(initial_state)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "answer": final_state.get("answer", ""),
                "sources": final_state.get("sources", []),
                "intent_info": final_state.get("intent_info", {}),
                "relevant_documents_count": len(final_state.get("relevant_documents", [])),
                "total_retrieved_count": len(final_state.get("retrieved_documents", [])),
                "confidence_score": final_state.get("confidence_score", 0.0),
                "processing_time": processing_time,
                "status": final_state.get("status", "unknown")
            }
        except Exception as e:
            return {
                "answer": f"L·ªói x·ª≠ l√Ω: {str(e)}",
                "sources": [],
                "intent_info": {"query_type": "error"},
                "relevant_documents_count": 0,
                "total_retrieved_count": 0,
                "confidence_score": 0.0,
                "processing_time": 0.0,
                "status": "error"
            }
    
    def detect_query_intent(self, query: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch intent c·ªßa c√¢u h·ªèi v·ªÅ m·∫Øt k√≠nh
        """
        intent_info = {
            "query_type": "general",
            "vision_condition": None,
            "product_interest": None,
            "style_preference": None,
            "technical_level": "basic"
        }
        
        query_lower = query.lower()
        
        # Detect vision conditions
        for condition in EYEWEAR_KEYWORDS["vision_conditions"]:
            if condition.lower() in query_lower:
                intent_info["vision_condition"] = condition
                intent_info["query_type"] = "medical_consultation"
                break
        
        # Detect product interest
        for product in EYEWEAR_KEYWORDS["lens_types"]:
            if product.lower() in query_lower:
                intent_info["product_interest"] = product
                intent_info["query_type"] = "product_recommendation"
                break
        
        # Detect style preference
        for style in EYEWEAR_KEYWORDS["frame_styles"]:
            if style.lower() in query_lower:
                intent_info["style_preference"] = style
                intent_info["query_type"] = "style_consultation"
                break
        
        # Detect technical questions
        technical_terms = ["coating", "l·ªõp ph·ªß", "ch·ªâ s·ªë kh√∫c x·∫°", "ƒë·ªô d√†y", "UV", "blue light"]
        if any(term in query_lower for term in technical_terms):
            intent_info["technical_level"] = "advanced"
        
        return intent_info
    
    def create_domain_prompt(self, query: str, context: str, intent_info: Dict) -> str:
        """
        T·∫°o prompt t·ªëi ∆∞u cho domain m·∫Øt k√≠nh
        """
        base_role = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n m·∫Øt k√≠nh chuy√™n nghi·ªáp v·ªõi nhi·ªÅu nƒÉm kinh nghi·ªám trong lƒ©nh v·ª±c quang h·ªçc. 
B·∫°n c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ:
- C√°c t·∫≠t kh√∫c x·∫° m·∫Øt (c·∫≠n th·ªã, vi·ªÖn th·ªã, lo·∫°n th·ªã, l√£o th·ªã)
- C√°c ki·∫øn th·ª©c s·ª©c kho·∫ª v·ªÅ m·∫Øt
- C√°c lo·∫°i tr√≤ng k√≠nh v√† c√¥ng ngh·ªá lens
- Phong c√°ch v√† thi·∫øt k·∫ø g·ªçng k√≠nh
- V·∫≠t li·ªáu v√† c√¥ng ngh·ªá s·∫£n xu·∫•t
- Xu h∆∞·ªõng th·ªùi trang k√≠nh m·∫Øt"""

        if intent_info["query_type"] == "medical_consultation":
            role_specific = """
H√£y t·∫≠p trung v√†o:
- Gi·∫£i th√≠ch r√µ r√†ng t√¨nh tr·∫°ng th·ªã l·ª±c
- ƒê·ªÅ xu·∫•t lo·∫°i tr√≤ng k√≠nh ph√π h·ª£p
- L∆∞u √Ω v·ªÅ vi·ªác thƒÉm kh√°m m·∫Øt ƒë·ªãnh k·ª≥
- Kh√¥ng thay th·∫ø √Ω ki·∫øn b√°c sƒ© chuy√™n khoa"""

        elif intent_info["query_type"] == "style_consultation":
            role_specific = """
H√£y t·∫≠p trung v√†o:
- Ph√¢n t√≠ch khu√¥n m·∫∑t v√† phong c√°ch ph√π h·ª£p
- Xu h∆∞·ªõng th·ªùi trang m·∫Øt k√≠nh
- C√°ch ph·ªëi h·ª£p v·ªõi trang ph·ª•c
- L·ªùi khuy√™n v·ªÅ m√†u s·∫Øc v√† ch·∫•t li·ªáu"""

        else:
            role_specific = """
H√£y cung c·∫•p th√¥ng tin to√†n di·ªán v√† th·ª±c t·∫ø."""

        context_instruction = f"""
D·ª±a tr√™n th√¥ng tin sau t·ª´ c∆° s·ªü d·ªØ li·ªáu v·ªÅ m·∫Øt k√≠nh:

{context}

C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {query}
"""

        response_guidelines = """
H√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c:
1. **Ph√¢n t√≠ch nhu c·∫ßu**: Hi·ªÉu r√µ t√¨nh hu·ªëng c·ªßa kh√°ch h√†ng
2. **ƒê·ªÅ xu·∫•t c·ª• th·ªÉ**: G·ª£i √Ω s·∫£n ph·∫©m/gi·∫£i ph√°p ph√π h·ª£p  
3. **Gi·∫£i th√≠ch l√Ω do**: T·∫°i sao ƒë√¢y l√† l·ª±a ch·ªçn t·ªët
4. **L∆∞u √Ω quan tr·ªçng**: ƒêi·ªÅu c·∫ßn ch√∫ √Ω khi s·ª≠ d·ª•ng
5. **T∆∞ v·∫•n th√™m**: G·ª£i √Ω v·ªÅ chƒÉm s√≥c ho·∫∑c l·ª±a ch·ªçn kh√°c

L∆∞u √Ω:
- S·ª≠ d·ª•ng ng√¥n ng·ªØ th√¢n thi·ªán, d·ªÖ hi·ªÉu
- ƒê∆∞a ra l·ªùi khuy√™n th·ª±c t·∫ø v√† c√≥ cƒÉn c·ª©
- N·∫øu c·∫ßn thƒÉm kh√°m chuy√™n khoa, h√£y khuy√™n kh√°ch h√†ng ƒëi kh√°m
- Tr√°nh cam k·∫øt ch·ªØa b·ªánh ho·∫∑c k·∫øt qu·∫£ ch·∫Øc ch·∫Øn
"""

        return f"{base_role}\n{role_specific}\n{context_instruction}\n{response_guidelines}"
    
    def enhance_context_with_keywords(self, context: str, intent_info: Dict) -> str:
        """
        TƒÉng c∆∞·ªùng context v·ªõi keywords domain-specific
        """
        enhanced_context = context
        
        # Th√™m keywords li√™n quan d·ª±a tr√™n intent
        if intent_info["vision_condition"]:
            related_keywords = [kw for kw in EYEWEAR_KEYWORDS["vision_conditions"] 
                              if kw != intent_info["vision_condition"]]
            enhanced_context += f"\n\nC√°c t√¨nh tr·∫°ng li√™n quan: {', '.join(related_keywords)}"
        
        if intent_info["product_interest"]:
            related_products = [kw for kw in EYEWEAR_KEYWORDS["lens_types"] 
                              if kw != intent_info["product_interest"]]
            enhanced_context += f"\n\nC√°c lo·∫°i s·∫£n ph·∫©m li√™n quan: {', '.join(related_products[:3])}"
        
        return enhanced_context
    
    def post_process_response(self, response: str, intent_info: Dict) -> str:
        """
        X·ª≠ l√Ω response ƒë·ªÉ ph√π h·ª£p v·ªõi domain
        """
        # Th√™m disclaimer cho medical advice
        if intent_info["query_type"] == "medical_consultation":
            disclaimer = "\n\n‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng**: Th√¥ng tin n√†y ch·ªâ mang t√≠nh tham kh·∫£o. H√£y thƒÉm kh√°m b√°c sƒ© nh√£n khoa ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n v√† t∆∞ v·∫•n ch√≠nh x√°c."
            if disclaimer not in response:
                response += disclaimer
        
        # Th√™m call-to-action cho product recommendations
        if intent_info["query_type"] == "product_recommendation":
            cta = "\n\nüí° **G·ª£i √Ω**: B·∫°n c√≥ th·ªÉ ƒë·∫øn c·ª≠a h√†ng ƒë·ªÉ th·ª≠ tr·ª±c ti·∫øp v√† nh·∫≠n t∆∞ v·∫•n chi ti·∫øt t·ª´ nh√¢n vi√™n chuy√™n nghi·ªáp."
            if "c·ª≠a h√†ng" not in response.lower():
                response += cta
        
        return response

    def grade_retrieved_documents(self, query: str, documents: List[Dict]) -> List[Dict]:
        """
        B∆∞·ªõc 3.3: ƒê√°nh Gi√° v√† L·ªçc K·∫øt Qu·∫£ Truy Xu·∫•t
        ƒê√°nh gi√° ƒë·ªô li√™n quan c·ªßa documents v·ªõi c√¢u h·ªèi
        """
        print("ƒêang ƒë√°nh gi√° ƒë·ªô li√™n quan c·ªßa documents...")
        
        relevant_docs = []
        
        for doc in documents:
            # Ki·ªÉm tra ƒëi·ªÉm similarity t·ª´ Qdrant
            if doc["score"] >= Config.SIMILARITY_THRESHOLD:
                # Th√™m ƒë√°nh gi√° LLM n·∫øu c·∫ßn (t√πy ch·ªçn)
                grade_prompt = f"""
                H√£y ƒë√°nh gi√° xem ƒëo·∫°n vƒÉn b·∫£n sau c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi kh√¥ng.
                
                C√¢u h·ªèi: {query}
                
                ƒêo·∫°n vƒÉn b·∫£n: {doc["content"][:500]}...
                
                Tr·∫£ l·ªùi ch·ªâ "YES" n·∫øu li√™n quan ho·∫∑c "NO" n·∫øu kh√¥ng li√™n quan.
                """
                
                try:
                    messages = [HumanMessage(content=grade_prompt)]
                    response = self.llm.invoke(messages)
                    grade = response.content.strip().upper()
                    
                    if grade == "YES":
                        relevant_docs.append(doc)
                        print(f"‚úì Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} ƒë∆∞·ª£c ch·∫•p nh·∫≠n (score: {doc['score']:.3f})")
                    else:
                        print(f"‚úó Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} b·ªã lo·∫°i b·ªè (kh√¥ng li√™n quan)")
                        
                except Exception as e:
                    # N·∫øu l·ªói LLM, ch·ªâ d·ª±a v√†o ƒëi·ªÉm similarity
                    relevant_docs.append(doc)
                    print(f"‚ö† L·ªói ƒë√°nh gi√° Gemini, gi·ªØ document d·ª±a tr√™n similarity score: {e}")
            else:
                print(f"‚úó Document t·ª´ {doc['source']} - chunk {doc['chunk_id']} b·ªã lo·∫°i b·ªè (score th·∫•p: {doc['score']:.3f})")
        
        print(f"ƒê√£ l·ªçc ƒë∆∞·ª£c {len(relevant_docs)}/{len(documents)} documents li√™n quan")
        return relevant_docs
    
    def aggregate_context(self, documents: List[Dict]) -> str:
        """
        B∆∞·ªõc 3.4: T·ªïng H·ª£p Ng·ªØ C·∫£nh (Context Aggregation)
        K·∫øt h·ª£p n·ªôi dung c√°c documents th√†nh context th·ªëng nh·∫•t
        """
        if not documents:
            return ""
        
        print("ƒêang t·ªïng h·ª£p ng·ªØ c·∫£nh t·ª´ documents...")
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source_info = f"[Ngu·ªìn: {doc['source']}, Ph·∫ßn {doc['chunk_id']}]"
            content = f"{source_info}\n{doc['content']}\n"
            context_parts.append(content)
        
        # S·ª≠a formatting ƒë·ªÉ context r√µ r√†ng h∆°n
        context = "\n" + ("="*50 + "\n").join(context_parts)
        
        print(f"ƒê√£ t·ªïng h·ª£p context t·ª´ {len(documents)} documents")
        return context
    
    def generate_answer(self, query: str, context: str) -> str:
        """
        B∆∞·ªõc 3.5: T·∫°o C√¢u Tr·∫£ L·ªùi v·ªõi Gemini LLM (Answer Generation with LLM)
        S·ª≠ d·ª•ng Google Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context
        """
        if not context.strip():
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."
        
        print("ƒêang t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Google Gemini...")
        
        # Gemini works better with a single comprehensive prompt
        full_prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

H∆∞·ªõng d·∫´n quan tr·ªçng:
1. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p
2. Kh√¥ng t·ª± suy di·ªÖn ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† d·ªÖ hi·ªÉu
5. Tr√≠ch d·∫´n ngu·ªìn khi c√≥ th·ªÉ (t√™n file, ph·∫ßn)
6. N·∫øu c√≥ nhi·ªÅu ngu·ªìn th√¥ng tin, h√£y t·ªïng h·ª£p m·ªôt c√°ch logic

Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu:
{context}

C√¢u h·ªèi: {query}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh tr√™n. ƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:
- C√¢u tr·∫£ l·ªùi ch√≠nh
- Ngu·ªìn tham kh·∫£o (n·∫øu c√≥)
"""
        
        try:
            messages = [HumanMessage(content=full_prompt)]
            response = self.llm.invoke(messages)
            answer = response.content.strip()
            
            print("‚úì ƒê√£ t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Gemini th√†nh c√¥ng")
            return answer
            
        except Exception as e:
            error_msg = f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Gemini: {str(e)}"
            print(f"‚úó {error_msg}")
            return f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {error_msg}"
    
    def process_query(self, query: str, retrieved_documents: List[Dict]) -> Dict:
        """
        X·ª≠ l√Ω ho√†n ch·ªânh m·ªôt query: ƒë√°nh gi√° docs -> t·ªïng h·ª£p context -> t·∫°o answer
        """
        # B∆∞·ªõc 3.3: ƒê√°nh gi√° v√† l·ªçc documents
        relevant_docs = self.grade_retrieved_documents(query, retrieved_documents)
        
        # B∆∞·ªõc 3.4: T·ªïng h·ª£p context
        context = self.aggregate_context(relevant_docs)
        
        # B∆∞·ªõc 3.5: T·∫°o c√¢u tr·∫£ l·ªùi
        answer = self.generate_answer(query, context)
        
        return {
            "query": query,
            "answer": answer,
            "relevant_documents_count": len(relevant_docs),
            "total_retrieved_count": len(retrieved_documents),
            "sources": list(set([doc["source"] for doc in relevant_docs])),
            "context": context
        }

    def get_health_status(self) -> Dict[str, Any]:
        """
        Ki·ªÉm tra tr·∫°ng th√°i health c·ªßa RAG agent
        """
        try:
            # Test simple query
            test_response = self.llm.invoke("Test connection")
            return {
                "status": "healthy",
                "model": Config.GEMINI_MODEL,
                "domain": Config.DOMAIN,
                "workflow_type": "LangGraph",
                "features": {
                    "intent_detection": True,
                    "domain_prompts": True,
                    "medical_disclaimer": True,
                    "langgraph_workflow": True,
                    "product_recommendations": getattr(Config, 'ENABLE_PRODUCT_RECOMMENDATIONS', True),
                    "technical_advice": getattr(Config, 'ENABLE_TECHNICAL_ADVICE', True)
                },
                "nodes": [
                    "detect_intent",
                    "retrieve_documents", 
                    "filter_documents",
                    "aggregate_context",
                    "generate_answer",
                    "post_process"
                ]
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "workflow_type": "LangGraph"
            }

    def generate_response(self, query: str, context: str) -> Dict[str, Any]:
        """
        T·∫°o response v·ªõi logic domain-specific cho m·∫Øt k√≠nh (legacy method for compatibility)
        """
        try:
            # Ph√¢n t√≠ch intent
            intent_info = self.detect_query_intent(query)
            
            # TƒÉng c∆∞·ªùng context
            enhanced_context = self.enhance_context_with_keywords(context, intent_info)
            
            # T·∫°o prompt t·ªëi ∆∞u
            prompt = self.create_domain_prompt(query, enhanced_context, intent_info)
            
            # G·ªçi LLM
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # Post-process
            final_answer = self.post_process_response(answer, intent_info)
            
            return {
                "answer": final_answer,
                "intent_info": intent_info,
                "status": "success"
            }
            
        except Exception as e:
            return {
                "answer": f"Xin l·ªói, t√¥i g·∫∑p kh√≥ khƒÉn khi x·ª≠ l√Ω c√¢u h·ªèi v·ªÅ m·∫Øt k√≠nh. L·ªói: {str(e)}",
                "intent_info": {"query_type": "error"},
                "status": "error",
                "error": str(e)
        } 