#!/usr/bin/env python3
"""
Enhanced LangGraph Workflow cho Eyewear Advisor Agent
- Advanced intent detection v√† routing
- Multi-step reasoning v·ªõi memory
- Dynamic parameter adjustment
- Sophisticated error handling v√† recovery
- State management v·ªõi history
"""

from typing import Dict, List, Any, TypedDict, Literal, Optional
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field
import time
import json

from utils.embedding_manager import EmbeddingManager
from utils.qdrant_manager import QdrantManager
from agents.rag_agent import RAGAgent
from config import Config

# Intent Detection Models
class QueryIntent(BaseModel):
    """Structured output for intent detection"""
    intent_type: Literal["medical_consultation", "product_recommendation", "technical_advice", "style_consultation", "general_inquiry", "complex_analysis"] = Field(
        description="Type of user query intent"
    )
    confidence: float = Field(description="Confidence score 0-1", ge=0, le=1)
    sub_intents: List[str] = Field(description="Secondary intent categories", default=[])
    complexity_level: Literal["simple", "moderate", "complex", "expert"] = Field(description="Query complexity")
    requires_multi_step: bool = Field(description="Whether query needs multi-step reasoning")
    key_entities: List[str] = Field(description="Important entities extracted from query", default=[])

class WorkflowState(TypedDict):
    """Enhanced state cho LangGraph workflow"""
    # Core input
    query: str
    user_context: Dict[str, Any]  # User profile, preferences, history
    
    # Intent analysis
    intent: Optional[QueryIntent]
    processing_strategy: str
    
    # Multi-step reasoning
    reasoning_steps: List[Dict[str, Any]]
    current_step: int
    sub_queries: List[str]
    
    # Retrieval & processing
    query_embedding: List[float]
    retrieved_documents: List[Dict]
    filtered_documents: List[Dict]
    context_documents: List[Dict]
    
    # Response generation
    intermediate_answers: List[str]
    final_answer: str
    sources: List[str]
    confidence_score: float
    
    # State management
    messages: List[BaseMessage]
    step_history: List[str]
    error_count: int
    retry_count: int
    
    # Metadata
    processing_time: float
    tokens_used: int
    status: str

class EnhancedEyewearWorkflow:
    """
    Enhanced LangGraph workflow v·ªõi advanced features:
    - Intelligent intent detection
    - Dynamic routing based on intent
    - Multi-step reasoning capabilities
    - Sophisticated error handling
    - State memory management
    """
    
    def __init__(self):
        print("üöÄ Initializing Enhanced LangGraph Workflow...")
        
        # Initialize components
        self.embedding_manager = EmbeddingManager()
        self.qdrant_manager = QdrantManager()
        self.rag_agent = RAGAgent()
        
        # Initialize LLM for intent detection
        self.intent_llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=0.1,  # Low temperature for consistent intent detection
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        # Initialize reasoning LLM
        self.reasoning_llm = ChatGoogleGenerativeAI(
            model=Config.GEMINI_MODEL,
            temperature=0.3,
            google_api_key=Config.GOOGLE_API_KEY
        )
        
        # Create workflow
        self.workflow = self._create_enhanced_workflow()
        self.compiled_workflow = self.workflow.compile()
        
        print("‚úÖ Enhanced LangGraph Workflow ready!")
    
    def _create_enhanced_workflow(self) -> StateGraph:
        """Create enhanced workflow graph with sophisticated routing"""
        workflow = StateGraph(WorkflowState)
        
        # Core processing nodes
        workflow.add_node("analyze_intent", self.analyze_intent_node)
        workflow.add_node("plan_strategy", self.plan_strategy_node)
        workflow.add_node("retrieve_context", self.retrieve_context_node)
        workflow.add_node("filter_documents", self.filter_documents_node)
        
        # Specialized processing nodes
        workflow.add_node("medical_consultation", self.medical_consultation_node)
        workflow.add_node("product_recommendation", self.product_recommendation_node)
        workflow.add_node("technical_analysis", self.technical_analysis_node)
        workflow.add_node("style_consultation", self.style_consultation_node)
        workflow.add_node("complex_reasoning", self.complex_reasoning_node)
        workflow.add_node("general_response", self.general_response_node)
        
        # Multi-step processing
        workflow.add_node("decompose_query", self.decompose_query_node)
        workflow.add_node("process_sub_query", self.process_sub_query_node)
        workflow.add_node("synthesize_answers", self.synthesize_answers_node)
        
        # Finalization
        workflow.add_node("finalize_response", self.finalize_response_node)
        workflow.add_node("handle_error", self.handle_error_node)
        
        # Set entry point
        workflow.set_entry_point("analyze_intent")
        
        # Intent analysis flow
        workflow.add_edge("analyze_intent", "plan_strategy")
        
        # Strategy planning with conditional routing
        workflow.add_conditional_edges(
            "plan_strategy",
            self.route_by_strategy,
            {
                "simple_retrieval": "retrieve_context",
                "multi_step": "decompose_query",
                "error": "handle_error"
            }
        )
        
        # Document processing flow
        workflow.add_edge("retrieve_context", "filter_documents")
        
        # Intent-based routing
        workflow.add_conditional_edges(
            "filter_documents", 
            self.route_by_intent,
            {
                "medical_consultation": "medical_consultation",
                "product_recommendation": "product_recommendation", 
                "technical_advice": "technical_analysis",
                "style_consultation": "style_consultation",
                "complex_analysis": "complex_reasoning",
                "general_inquiry": "general_response"
            }
        )
        
        # Multi-step processing flow
        workflow.add_edge("decompose_query", "process_sub_query")
        workflow.add_conditional_edges(
            "process_sub_query",
            self.check_sub_queries_complete,
            {
                "continue": "process_sub_query",
                "synthesize": "synthesize_answers"
            }
        )
        
        # Finalization flow
        workflow.add_edge("medical_consultation", "finalize_response")
        workflow.add_edge("product_recommendation", "finalize_response")
        workflow.add_edge("technical_analysis", "finalize_response")
        workflow.add_edge("style_consultation", "finalize_response")
        workflow.add_edge("complex_reasoning", "finalize_response")
        workflow.add_edge("general_response", "finalize_response")
        workflow.add_edge("synthesize_answers", "finalize_response")
        
        # End states
        workflow.add_edge("finalize_response", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    def analyze_intent_node(self, state: WorkflowState) -> WorkflowState:
        """Advanced intent analysis with LLM"""
        try:
            print("üß† Analyzing query intent...")
            start_time = time.time()
            
            # Intent detection prompt
            intent_prompt = PromptTemplate(
                input_variables=["query"],
                template="""
Ph√¢n t√≠ch c√¢u h·ªèi sau v·ªÅ m·∫Øt k√≠nh v√† x√°c ƒë·ªãnh intent:

C√¢u h·ªèi: {query}

H√£y ph√¢n lo·∫°i intent theo c√°c categories:
1. medical_consultation: C√¢u h·ªèi v·ªÅ s·ª©c kh·ªèe m·∫Øt, b·ªánh l√Ω, ƒëi·ªÅu tr·ªã
2. product_recommendation: T√¨m hi·ªÉu/so s√°nh s·∫£n ph·∫©m m·∫Øt k√≠nh
3. technical_advice: Th√¥ng tin k·ªπ thu·∫≠t v·ªÅ tr√≤ng k√≠nh, c√¥ng ngh·ªá
4. style_consultation: T∆∞ v·∫•n phong c√°ch, th·∫©m m·ªπ
5. general_inquiry: C√¢u h·ªèi chung chung
6. complex_analysis: Y√™u c·∫ßu ph√¢n t√≠ch ph·ª©c t·∫°p, nhi·ªÅu b∆∞·ªõc

C≈©ng ƒë√°nh gi√°:
- ƒê·ªô ph·ª©c t·∫°p: simple/moderate/complex/expert
- C√≥ c·∫ßn multi-step reasoning kh√¥ng?
- Confidence score (0-1)
- Key entities trong c√¢u h·ªèi

Tr·∫£ v·ªÅ JSON format v·ªõi c√°c fields: intent_type, confidence, sub_intents, complexity_level, requires_multi_step, key_entities
"""
            )
            
            # Call LLM for intent detection
            prompt_formatted = intent_prompt.format(query=state["query"])
            messages = [SystemMessage(content="You are an expert intent classifier for eyewear domain."),
                       HumanMessage(content=prompt_formatted)]
            
            response = self.intent_llm.invoke(messages)
            
            # Parse response (simplified - in production use structured output)
            try:
                # Extract JSON from response (basic parsing)
                content = response.content
                if "```json" in content:
                    json_part = content.split("```json")[1].split("```")[0]
                elif "{" in content and "}" in content:
                    start_idx = content.find("{")
                    end_idx = content.rfind("}") + 1
                    json_part = content[start_idx:end_idx]
                else:
                    # Fallback
                    json_part = '{"intent_type": "general_inquiry", "confidence": 0.7, "sub_intents": [], "complexity_level": "moderate", "requires_multi_step": false, "key_entities": []}'
                
                intent_data = json.loads(json_part)
                intent = QueryIntent(**intent_data)
                
            except:
                # Fallback intent
                intent = QueryIntent(
                    intent_type="general_inquiry",
                    confidence=0.7,
                    complexity_level="moderate",
                    requires_multi_step=False
                )
            
            state["intent"] = intent
            state["step_history"] = ["analyze_intent"]
            state["processing_time"] = time.time() - start_time
            
            print(f"üéØ Intent detected: {intent.intent_type} (confidence: {intent.confidence:.2f})")
            print(f"üîç Complexity: {intent.complexity_level}, Multi-step: {intent.requires_multi_step}")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["status"] = "error"
            print(f"‚ùå Error in intent analysis: {e}")
            return state
    
    def plan_strategy_node(self, state: WorkflowState) -> WorkflowState:
        """Plan processing strategy based on intent analysis"""
        try:
            print("üìã Planning processing strategy...")
            
            intent = state["intent"]
            
            if intent.requires_multi_step or intent.complexity_level in ["complex", "expert"]:
                state["processing_strategy"] = "multi_step"
                print("üîÑ Strategy: Multi-step reasoning")
            else:
                state["processing_strategy"] = "simple_retrieval"
                print("‚û°Ô∏è  Strategy: Simple retrieval")
            
            state["step_history"].append("plan_strategy")
            return state
            
        except Exception as e:
            state["processing_strategy"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def retrieve_context_node(self, state: WorkflowState) -> WorkflowState:
        """Enhanced document retrieval with dynamic parameters"""
        try:
            print("üîç Retrieving relevant context...")
            
            # Create query embedding
            query_embedding = self.embedding_manager.embed_query(state["query"])
            state["query_embedding"] = query_embedding.tolist()
            
            # Dynamic retrieval parameters based on intent
            intent = state["intent"]
            if intent.complexity_level == "expert":
                top_k = min(15, Config.TOP_K_DOCUMENTS + 5)
            elif intent.complexity_level == "complex":
                top_k = min(12, Config.TOP_K_DOCUMENTS + 2)
            else:
                top_k = Config.TOP_K_DOCUMENTS
            
            # Retrieve documents
            retrieved_docs = self.qdrant_manager.search_similar_documents(
                query_embedding.tolist(), 
                top_k=top_k
            )
            
            state["retrieved_documents"] = retrieved_docs
            state["step_history"].append("retrieve_context")
            
            print(f"üìÑ Retrieved {len(retrieved_docs)} documents")
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["status"] = "error"
            return state
    
    def filter_documents_node(self, state: WorkflowState) -> WorkflowState:
        """Intelligent document filtering using LLM"""
        try:
            print("üéØ Filtering documents by relevance...")
            
            intent = state["intent"]
            retrieved_docs = state["retrieved_documents"]
            
            if not retrieved_docs:
                state["filtered_documents"] = []
                return state
            
            # LLM-based relevance filtering
            filter_prompt = f"""
ƒê√°nh gi√° relevance c·ªßa documents sau cho c√¢u h·ªèi: "{state['query']}"
Intent type: {intent.intent_type}
Key entities: {intent.key_entities}

Ch·ªâ gi·ªØ l·∫°i documents th·ª±c s·ª± h·ªØu √≠ch cho intent n√†y.
Tr·∫£ v·ªÅ danh s√°ch index c·ªßa documents relevance (t·ª´ 0 ƒë·∫øn {len(retrieved_docs)-1}).
"""
            
            # Simplified filtering (can be enhanced with LLM scoring)
            filtered_docs = retrieved_docs[:8]  # Basic filtering
            
            state["filtered_documents"] = filtered_docs
            state["step_history"].append("filter_documents")
            
            print(f"‚úÖ Filtered to {len(filtered_docs)} most relevant documents")
            return state
            
        except Exception as e:
            state["filtered_documents"] = state.get("retrieved_documents", [])
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    # Specialized processing nodes
    def medical_consultation_node(self, state: WorkflowState) -> WorkflowState:
        """Specialized medical consultation processing"""
        try:
            print("üè• Processing medical consultation...")
            
            medical_prompt = """
B·∫°n l√† chuy√™n gia nh√£n khoa v·ªõi 20+ nƒÉm kinh nghi·ªám. H√£y t∆∞ v·∫•n v·ªÅ v·∫•n ƒë·ªÅ m·∫Øt/k√≠nh d·ª±a tr√™n th√¥ng tin sau:

Context: {context}
C√¢u h·ªèi: {query}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Ph√¢n t√≠ch tri·ªáu ch·ª©ng/v·∫•n ƒë·ªÅ n·∫øu c√≥
2. ƒê∆∞a ra l·ªùi khuy√™n chuy√™n m√¥n
3. LU√îN khuy·∫øn ngh·ªã thƒÉm kh√°m b√°c sƒ© nh√£n khoa n·∫øu li√™n quan s·ª©c kh·ªèe
4. Cung c·∫•p th√¥ng tin an to√†n v√† c√≥ cƒÉn c·ª©
5. Tr√°nh ch·∫©n ƒëo√°n qua internet

L∆∞u √Ω: ∆Øu ti√™n s·ª± an to√†n c·ªßa b·ªánh nh√¢n.
"""
            
            result = self.rag_agent.process_query(
                state["query"], 
                state["filtered_documents"],
                custom_prompt=medical_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.9  # High confidence for medical
            state["step_history"].append("medical_consultation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω t∆∞ v·∫•n y t·∫ø. Vui l√≤ng thƒÉm kh√°m b√°c sƒ© nh√£n khoa."
            return state
    
    def product_recommendation_node(self, state: WorkflowState) -> WorkflowState:
        """Specialized product recommendation processing"""
        try:
            print("üõçÔ∏è Processing product recommendation...")
            
            product_prompt = """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n s·∫£n ph·∫©m m·∫Øt k√≠nh v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ c√°c th∆∞∆°ng hi·ªáu v√† c√¥ng ngh·ªá.

Context: {context}
C√¢u h·ªèi: {query}

H√£y ƒë∆∞a ra khuy·∫øn ngh·ªã s·∫£n ph·∫©m:
1. Ph√¢n t√≠ch nhu c·∫ßu c·ªßa kh√°ch h√†ng
2. So s√°nh c√°c l·ª±a ch·ªçn ph√π h·ª£p
3. Gi·∫£i th√≠ch ∆∞u/nh∆∞·ª£c ƒëi·ªÉm t·ª´ng lo·∫°i
4. ƒê∆∞a ra khuy·∫øn ngh·ªã c·ª• th·ªÉ v·ªõi l√Ω do
5. ƒê·ªÅ xu·∫•t m·ª©c gi√° v√† n∆°i mua n·∫øu c√≥
6. L∆∞u √Ω v·ªÅ t∆∞∆°ng th√≠ch v·ªõi ƒë·∫∑c ƒëi·ªÉm c√° nh√¢n

Phong c√°ch: Th√¢n thi·ªán, chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"], 
                custom_prompt=product_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.85
            state["step_history"].append("product_recommendation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi ƒë∆∞a ra khuy·∫øn ngh·ªã s·∫£n ph·∫©m."
            return state
    
    def technical_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """Technical analysis processing"""
        try:
            print("üî¨ Processing technical analysis...")
            
            technical_prompt = """
B·∫°n l√† k·ªπ s∆∞ quang h·ªçc chuy√™n v·ªÅ c√¥ng ngh·ªá tr√≤ng k√≠nh v√† g·ªçng m·∫Øt k√≠nh.

Context: {context}
C√¢u h·ªèi: {query}

H√£y ƒë∆∞a ra ph√¢n t√≠ch k·ªπ thu·∫≠t:
1. Gi·∫£i th√≠ch c√°c kh√°i ni·ªám/c√¥ng ngh·ªá li√™n quan
2. Ph√¢n t√≠ch ∆∞u/nh∆∞·ª£c ƒëi·ªÉm k·ªπ thu·∫≠t
3. So s√°nh v·ªõi c√°c c√¥ng ngh·ªá kh√°c
4. ƒê∆∞a ra ƒë√°nh gi√° kh√°ch quan
5. Gi·∫£i th√≠ch c√°ch ho·∫°t ƒë·ªông n·∫øu ph√π h·ª£p
6. ƒê·ªÅ xu·∫•t ·ª©ng d·ª•ng th·ª±c t·∫ø

Phong c√°ch: Ch√≠nh x√°c, khoa h·ªçc, chi ti·∫øt nh∆∞ng d·ªÖ hi·ªÉu.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=technical_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.88
            state["step_history"].append("technical_analysis")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi ph√¢n t√≠ch k·ªπ thu·∫≠t."
            return state
    
    def style_consultation_node(self, state: WorkflowState) -> WorkflowState:
        """Style consultation processing"""
        try:
            print("üëì Processing style consultation...")
            
            style_prompt = """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n phong c√°ch v√† th·∫©m m·ªπ m·∫Øt k√≠nh v·ªõi kinh nghi·ªám styling cho nhi·ªÅu d√°ng m·∫∑t.

Context: {context}
C√¢u h·ªèi: {query}

H√£y t∆∞ v·∫•n phong c√°ch:
1. Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm khu√¥n m·∫∑t/phong c√°ch c√° nh√¢n
2. Khuy·∫øn ngh·ªã ki·ªÉu g·ªçng ph√π h·ª£p
3. T∆∞ v·∫•n v·ªÅ m√†u s·∫Øc v√† ch·∫•t li·ªáu
4. ƒê∆∞a ra tips ph·ªëi ƒë·ªì v·ªõi m·∫Øt k√≠nh
5. G·ª£i √Ω xu h∆∞·ªõng th·ªùi trang m·∫Øt k√≠nh
6. C√¢n nh·∫Øc v·ªÅ t√≠nh th·ª±c t·∫ø v√† s·ª≠ d·ª•ng

Phong c√°ch: Th√¢n thi·ªán, s√°ng t·∫°o, c·∫≠p nh·∫≠t xu h∆∞·ªõng.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=style_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.82
            state["step_history"].append("style_consultation")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t∆∞ v·∫•n phong c√°ch."
            return state
    
    def complex_reasoning_node(self, state: WorkflowState) -> WorkflowState:
        """Complex multi-faceted analysis"""
        try:
            print("üß© Processing complex analysis...")
            
            complex_prompt = """
B·∫°n l√† chuy√™n gia t·ªïng h·ª£p v·ªõi ki·∫øn th·ª©c ƒëa ng√†nh v·ªÅ m·∫Øt k√≠nh (y t·∫ø, k·ªπ thu·∫≠t, th·∫©m m·ªπ, kinh t·∫ø).

Context: {context}
C√¢u h·ªèi ph·ª©c t·∫°p: {query}

H√£y ƒë∆∞a ra ph√¢n t√≠ch to√†n di·ªán:
1. Ph√¢n t√≠ch ƒëa chi·ªÅu v·∫•n ƒë·ªÅ
2. Xem x√©t c√°c y·∫øu t·ªë li√™n quan (s·ª©c kh·ªèe, kinh t·∫ø, th·∫©m m·ªπ, k·ªπ thu·∫≠t)
3. ƒê∆∞a ra pros/cons c·ªßa c√°c l·ª±a ch·ªçn
4. Khuy·∫øn ngh·ªã t·ªëi ∆∞u d·ª±a tr√™n trade-offs
5. ƒê·ªÅ xu·∫•t c√°c b∆∞·ªõc th·ª±c hi·ªán c·ª• th·ªÉ
6. C√¢n nh·∫Øc r·ªßi ro v√† l∆∞u √Ω

Phong c√°ch: To√†n di·ªán, c√¢n b·∫±ng, c√≥ c·∫•u tr√∫c r√µ r√†ng.
"""
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"],
                custom_prompt=complex_prompt
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.87
            state["step_history"].append("complex_reasoning")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi ph√¢n t√≠ch ph·ª©c t·∫°p."
            return state
    
    def general_response_node(self, state: WorkflowState) -> WorkflowState:
        """General response processing"""
        try:
            print("üí¨ Processing general inquiry...")
            
            result = self.rag_agent.process_query(
                state["query"],
                state["filtered_documents"]
            )
            
            state["final_answer"] = result["answer"]
            state["sources"] = result.get("sources", [])
            state["confidence_score"] = 0.75
            state["step_history"].append("general_response")
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            state["final_answer"] = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi."
            return state
    
    # Multi-step processing nodes
    def decompose_query_node(self, state: WorkflowState) -> WorkflowState:
        """Decompose complex query into sub-queries"""
        try:
            print("üîÑ Decomposing complex query...")
            
            # Use LLM to break down complex query
            decompose_prompt = f"""
Ph√¢n t√°ch c√¢u h·ªèi ph·ª©c t·∫°p sau th√†nh c√°c sub-queries ƒë∆°n gi·∫£n h∆°n:

Query: {state['query']}
Intent: {state['intent'].intent_type}

Chia th√†nh 2-4 c√¢u h·ªèi con c√≥ th·ªÉ tr·∫£ l·ªùi ƒë·ªôc l·∫≠p:
1. [Sub-query 1]
2. [Sub-query 2]
...

Tr·∫£ v·ªÅ danh s√°ch c√°c sub-queries.
"""
            
            # Simplified decomposition (can enhance with LLM)
            sub_queries = [
                f"Th√¥ng tin c∆° b·∫£n v·ªÅ {' '.join(state['intent'].key_entities)}",
                f"Khuy·∫øn ngh·ªã c·ª• th·ªÉ cho {state['query'][:50]}...",
                "L∆∞u √Ω v√† c√¢n nh·∫Øc quan tr·ªçng"
            ]
            
            state["sub_queries"] = sub_queries
            state["current_step"] = 0
            state["intermediate_answers"] = []
            state["step_history"].append("decompose_query")
            
            print(f"üß© Decomposed into {len(sub_queries)} sub-queries")
            return state
            
        except Exception as e:
            state["processing_strategy"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def process_sub_query_node(self, state: WorkflowState) -> WorkflowState:
        """Process individual sub-query"""
        try:
            current_step = state["current_step"]
            sub_queries = state["sub_queries"]
            
            if current_step < len(sub_queries):
                sub_query = sub_queries[current_step]
                print(f"üîç Processing sub-query {current_step + 1}: {sub_query}")
                
                # Process sub-query with relevant documents
                result = self.rag_agent.process_query(
                    sub_query,
                    state["filtered_documents"]
                )
                
                state["intermediate_answers"].append({
                    "sub_query": sub_query,
                    "answer": result["answer"],
                    "sources": result.get("sources", [])
                })
                
                state["current_step"] += 1
            
            return state
            
        except Exception as e:
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def synthesize_answers_node(self, state: WorkflowState) -> WorkflowState:
        """Synthesize multiple answers into comprehensive response"""
        try:
            print("üîó Synthesizing comprehensive answer...")
            
            intermediate_answers = state["intermediate_answers"]
            
            # Create synthesis prompt
            synthesis_content = "\n\n".join([
                f"Q: {ans['sub_query']}\nA: {ans['answer']}"
                for ans in intermediate_answers
            ])
            
            synthesis_prompt = f"""
T·ªïng h·ª£p c√°c c√¢u tr·∫£ l·ªùi sau th√†nh m·ªôt ph·∫£n h·ªìi ho√†n ch·ªânh cho c√¢u h·ªèi g·ªëc:

C√¢u h·ªèi g·ªëc: {state['query']}

C√°c c√¢u tr·∫£ l·ªùi th√†nh ph·∫ßn:
{synthesis_content}

H√£y t·∫°o m·ªôt c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p:
1. C√≥ c·∫•u tr√∫c r√µ r√†ng
2. Lo·∫°i b·ªè th√¥ng tin tr√πng l·∫∑p
3. K·∫øt n·ªëi logic gi·ªØa c√°c ph·∫ßn
4. ƒê∆∞a ra k·∫øt lu·∫≠n t·ªïng th·ªÉ
5. Gi·ªØ nguy√™n c√°c th√¥ng tin quan tr·ªçng
"""
            
            # Use reasoning LLM for synthesis
            messages = [
                SystemMessage(content="You are an expert at synthesizing information."),
                HumanMessage(content=synthesis_prompt)
            ]
            
            response = self.reasoning_llm.invoke(messages)
            
            state["final_answer"] = response.content
            
            # Collect all sources
            all_sources = []
            for ans in intermediate_answers:
                all_sources.extend(ans.get("sources", []))
            state["sources"] = list(set(all_sources))  # Remove duplicates
            
            state["confidence_score"] = 0.85
            state["step_history"].append("synthesize_answers")
            
            return state
            
        except Exception as e:
            # Fallback to simple concatenation
            answers = [ans["answer"] for ans in state["intermediate_answers"]]
            state["final_answer"] = "\n\n".join(answers)
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def finalize_response_node(self, state: WorkflowState) -> WorkflowState:
        """Finalize and enhance the response"""
        try:
            print("‚ú® Finalizing response...")
            
            # Add metadata and enhancements
            if not state.get("final_answer"):
                state["final_answer"] = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
                state["confidence_score"] = 0.0
            
            # Ensure we have sources
            if not state.get("sources"):
                state["sources"] = []
            
            # Calculate total processing time
            total_time = time.time() - state.get("processing_time", time.time())
            state["processing_time"] = total_time
            
            state["status"] = "completed"
            state["step_history"].append("finalize_response")
            
            print(f"‚úÖ Response finalized in {total_time:.2f}s")
            return state
            
        except Exception as e:
            state["status"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1
            return state
    
    def handle_error_node(self, state: WorkflowState) -> WorkflowState:
        """Enhanced error handling with recovery"""
        try:
            error_count = state.get("error_count", 0)
            
            if error_count < 3:  # Retry logic
                print(f"‚ö†Ô∏è Error occurred, attempting recovery (attempt {error_count + 1}/3)")
                state["final_answer"] = "ƒêang th·ª≠ l·∫°i x·ª≠ l√Ω c√¢u h·ªèi..."
                state["retry_count"] = state.get("retry_count", 0) + 1
            else:
                print("‚ùå Maximum errors reached, providing fallback response")
                state["final_answer"] = """
Xin l·ªói, t√¥i g·∫∑p kh√≥ khƒÉn khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. 

Vui l√≤ng:
1. Th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ƒë∆°n gi·∫£n h∆°n
2. Ki·ªÉm tra k·∫øt n·ªëi internet
3. Li√™n h·ªá h·ªó tr·ª£ k·ªπ thu·∫≠t n·∫øu v·∫•n ƒë·ªÅ ti·∫øp t·ª•c

C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª• t∆∞ v·∫•n m·∫Øt k√≠nh!
"""
            
            state["status"] = "error_handled"
            state["confidence_score"] = 0.1
            state["sources"] = []
            
            return state
            
        except Exception as e:
            state["final_answer"] = "H·ªá th·ªëng g·∫∑p l·ªói nghi√™m tr·ªçng. Vui l√≤ng th·ª≠ l·∫°i sau."
            state["status"] = "critical_error"
            return state
    
    # Routing functions
    def route_by_strategy(self, state: WorkflowState) -> str:
        """Route based on processing strategy"""
        strategy = state.get("processing_strategy", "simple_retrieval")
        
        if strategy == "error":
            return "error"
        elif strategy == "multi_step":
            return "multi_step"
        else:
            return "simple_retrieval"
    
    def route_by_intent(self, state: WorkflowState) -> str:
        """Route based on detected intent"""
        intent = state.get("intent")
        if not intent:
            return "general_inquiry"
        
        return intent.intent_type
    
    def check_sub_queries_complete(self, state: WorkflowState) -> str:
        """Check if all sub-queries are processed"""
        current_step = state.get("current_step", 0)
        total_steps = len(state.get("sub_queries", []))
        
        if current_step < total_steps:
            return "continue"
        else:
            return "synthesize"
    
    # Main execution methods
    def invoke(self, query: str, user_context: Dict = None, **kwargs) -> Dict:
        """
        Main invoke method for the enhanced workflow
        
        Args:
            query: User question
            user_context: User profile/preferences/history
            **kwargs: Additional parameters
        
        Returns:
            Comprehensive result dictionary
        """
        try:
            # Initialize state
            initial_state = WorkflowState(
                query=query,
                user_context=user_context or {},
                reasoning_steps=[],
                current_step=0,
                sub_queries=[],
                retrieved_documents=[],
                filtered_documents=[],
                intermediate_answers=[],
                messages=[],
                step_history=[],
                error_count=0,
                retry_count=0,
                processing_time=time.time(),
                tokens_used=0,
                status="processing"
            )
            
            # Run workflow
            final_state = self.compiled_workflow.invoke(initial_state)
            
            # Format response
            return {
                "query": query,
                "answer": final_state.get("final_answer", ""),
                "sources": final_state.get("sources", []),
                "intent": final_state.get("intent").dict() if final_state.get("intent") else None,
                "confidence_score": final_state.get("confidence_score", 0.0),
                "processing_time": final_state.get("processing_time", 0.0),
                "step_history": final_state.get("step_history", []),
                "status": final_state.get("status", "unknown"),
                "metadata": {
                    "workflow_type": "enhanced_langgraph",
                    "error_count": final_state.get("error_count", 0),
                    "retry_count": final_state.get("retry_count", 0),
                    "reasoning_steps": len(final_state.get("reasoning_steps", [])),
                    "documents_retrieved": len(final_state.get("retrieved_documents", [])),
                    "documents_filtered": len(final_state.get("filtered_documents", []))
                }
            }
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"L·ªói nghi√™m tr·ªçng trong workflow: {str(e)}",
                "sources": [],
                "intent": None,
                "confidence_score": 0.0,
                "processing_time": 0.0,
                "step_history": ["error"],
                "status": "critical_error",
                "error": str(e),
                "metadata": {"workflow_type": "enhanced_langgraph"}
            }

def create_enhanced_workflow() -> EnhancedEyewearWorkflow:
    """Factory function to create enhanced workflow"""
    return EnhancedEyewearWorkflow()

def get_enhanced_workflow() -> EnhancedEyewearWorkflow:
    """Get singleton instance of enhanced workflow"""
    global _enhanced_workflow_instance
    if '_enhanced_workflow_instance' not in globals():
        _enhanced_workflow_instance = create_enhanced_workflow()
    return _enhanced_workflow_instance 